{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAIEdWJlhynYdhEZl54eLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msdevanms/pyspark_training/blob/main/Spark_wordcount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn4NE3TP768r",
        "outputId": "d660d00e-408d-437e-9dcb-ac88c167ef64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhbKr8NI8N1M",
        "outputId": "e0c201ee-4d0b-4771-dd92-70cf48e66726"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=8ed1f2ec324fb40f4aeba6cd70a81a128a4c3089ce9a030b8f7ff95692c21f64\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SparkSession and sparkcontext\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "                    .master(\"local\")\\\n",
        "                    .appName('Firstprogram')\\\n",
        "                    .getOrCreate()\n",
        "sc=spark.sparkContext"
      ],
      "metadata": {
        "id": "eQdv0ddS8qu9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the input file and Calculating words count\n",
        "text_file = sc.textFile(\"sample_data/README.md\")\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "                            .map(lambda word: (word, 1)) \\\n",
        "                           .reduceByKey(lambda x, y: x + y)"
      ],
      "metadata": {
        "id": "xOBRUvhP8xBe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing each word with its respective count\n",
        "output = counts.collect()\n",
        "for (word, count) in output:\n",
        "    print(\"%s: %i\" % (word, count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3LByY5v861g",
        "outputId": "ea95dec7-3274-4aa0-abea-967a067cd94f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This: 1\n",
            "directory: 1\n",
            "includes: 1\n",
            "a: 3\n",
            "few: 1\n",
            "sample: 2\n",
            "datasets: 1\n",
            "to: 1\n",
            "get: 1\n",
            "you: 1\n",
            "started.: 1\n",
            ": 51\n",
            "*: 3\n",
            "`california_housing_data*.csv`: 1\n",
            "is: 4\n",
            "California: 1\n",
            "housing: 1\n",
            "data: 1\n",
            "from: 1\n",
            "the: 3\n",
            "1990: 1\n",
            "US: 1\n",
            "Census;: 1\n",
            "more: 1\n",
            "information: 1\n",
            "available: 1\n",
            "at:: 2\n",
            "https://developers.google.com/machine-learning/crash-course/california-housing-data-description: 1\n",
            "`mnist_*.csv`: 1\n",
            "small: 1\n",
            "of: 2\n",
            "[MNIST: 1\n",
            "database](https://en.wikipedia.org/wiki/MNIST_database),: 1\n",
            "which: 1\n",
            "described: 2\n",
            "http://yann.lecun.com/exdb/mnist/: 1\n",
            "`anscombe.json`: 1\n",
            "contains: 1\n",
            "copy: 2\n",
            "[Anscombe's: 1\n",
            "quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet);: 1\n",
            "it: 1\n",
            "was: 2\n",
            "originally: 1\n",
            "in: 2\n",
            "Anscombe,: 1\n",
            "F.: 1\n",
            "J.: 1\n",
            "(1973).: 1\n",
            "'Graphs: 1\n",
            "Statistical: 1\n",
            "Analysis'.: 1\n",
            "American: 1\n",
            "Statistician.: 1\n",
            "27: 1\n",
            "(1):: 1\n",
            "17-21.: 1\n",
            "JSTOR: 1\n",
            "2682899.: 1\n",
            "and: 1\n",
            "our: 1\n",
            "prepared: 1\n",
            "by: 1\n",
            "[vega_datasets: 1\n",
            "library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts_sorted = counts.sortBy(lambda x: x[1])"
      ],
      "metadata": {
        "id": "1hlgjziw_428"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_sorted = counts_sorted.collect()\n",
        "for (word, count) in output_sorted:\n",
        "    print(\"%s: %i\" % (word, count))"
      ],
      "metadata": {
        "id": "m2C8ZppFArYI",
        "outputId": "aaa321ce-becd-49b1-c034-b00c3770898d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This: 1\n",
            "directory: 1\n",
            "includes: 1\n",
            "few: 1\n",
            "datasets: 1\n",
            "to: 1\n",
            "get: 1\n",
            "you: 1\n",
            "started.: 1\n",
            "`california_housing_data*.csv`: 1\n",
            "California: 1\n",
            "housing: 1\n",
            "data: 1\n",
            "from: 1\n",
            "1990: 1\n",
            "US: 1\n",
            "Census;: 1\n",
            "more: 1\n",
            "information: 1\n",
            "available: 1\n",
            "https://developers.google.com/machine-learning/crash-course/california-housing-data-description: 1\n",
            "`mnist_*.csv`: 1\n",
            "small: 1\n",
            "[MNIST: 1\n",
            "database](https://en.wikipedia.org/wiki/MNIST_database),: 1\n",
            "which: 1\n",
            "http://yann.lecun.com/exdb/mnist/: 1\n",
            "`anscombe.json`: 1\n",
            "contains: 1\n",
            "[Anscombe's: 1\n",
            "quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet);: 1\n",
            "it: 1\n",
            "originally: 1\n",
            "Anscombe,: 1\n",
            "F.: 1\n",
            "J.: 1\n",
            "(1973).: 1\n",
            "'Graphs: 1\n",
            "Statistical: 1\n",
            "Analysis'.: 1\n",
            "American: 1\n",
            "Statistician.: 1\n",
            "27: 1\n",
            "(1):: 1\n",
            "17-21.: 1\n",
            "JSTOR: 1\n",
            "2682899.: 1\n",
            "and: 1\n",
            "our: 1\n",
            "prepared: 1\n",
            "by: 1\n",
            "[vega_datasets: 1\n",
            "library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).: 1\n",
            "sample: 2\n",
            "at:: 2\n",
            "of: 2\n",
            "described: 2\n",
            "copy: 2\n",
            "was: 2\n",
            "in: 2\n",
            "a: 3\n",
            "the: 3\n",
            "is: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the input file and Calculating words count\n",
        "text_file = sc.textFile(\"sample_data/README.md\")\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "                            .filter(lambda word: (word not in [\",\",\"*\", \"'\",\" \",\"\"])) \\\n",
        "                            .map(lambda word: (word, 1)) \\\n",
        "                           .reduceByKey(lambda x, y: x + y)"
      ],
      "metadata": {
        "id": "qblX6PAJCXje"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}